# Awesome_Visually-Augmented_NLP

üñºÔ∏èLatest Papers on Visually(Imagination)-Augmented NLP

Visually(Imagination)-augmented NLP here refers to the field of research on **solving natural language processing tasks (mainly uni-modal language generation) using relevant images inside the model to improve the accuracy of the task**.

Unlike Multimodal NLP and Vision-and-Language, the only input/output to the model is text, and images related to the input are retrieved or generated inside the model and used to generate text for the output.

<p align="center">
    <a href="https://awesome.re">
        <img alt="awesome", src="https://awesome.re/badge.svg">
    </a>
</p>

<p align="center">
  <img src="./example.png" width="50%">
</p>

| Title                                                        | Link                                              | Date       | Code                                                   |
| :----------------------------------------------------------- | :------------------------------------------------ | :--------- | :----------------------------------------------------- |
| Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics | https://arxiv.org/abs/2309.07120                  | 2023/09/13 | [Code](https://github.com/UCSC-VLAA/Sight-Beyond-Text) |
| Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models | https://arxiv.org/abs/2308.16463                  | 2023/08/31 | [Code](https://github.com/HYPJUDY/Sparkles)            |
| Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems | https://arxiv.org/abs/2308.10354                  | 2023/08/20 |                                                        |
| ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation | https://arxiv.org/abs/2308.00400                  | 2023/08/01 | [Code](https://github.com/zhangbo-nlp/ZRIGF)           |
| Visually-Enhanced Phrase Understanding                       | https://aclanthology.org/2023.findings-acl.363/   | 2023/07    |                                                        |
| Learning to Imagine: Visually-Augmented Natural Language Generation | https://arxiv.org/abs/2305.16944                  | 2023/05/26 | [Code](https://github.com/rucaibox/live)               |
| ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue | https://arxiv.org/abs/2305.13602                  | 2023/05/23 | [Code](https://github.com/ImKeTT/ReSee)                |
| Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained Language Models with Cross-Modal Adapters | https://arxiv.org/abs/2305.07358                  | 2023/05/12 |                                                        |
| Visualize Before You Write: Imagination-Guided Open-Ended Text Generation | https://aclanthology.org/2023.findings-eacl.5/    | 2023/05    |                                                        |
| Visually-augmented pretrained language models for NLP tasks without images | https://arxiv.org/abs/2212.07937                  | 2022/12/15 | [Code](https://github.com/rucaibox/vawi)               |
| Think Beyond Words: Exploring Context-Relevant Visual Commonsense for Diverse Dialogue Generation | https://aclanthology.org/2022.findings-emnlp.226/ | 2022/12    |                                                        |
| Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination | https://arxiv.org/abs/2210.12261                  | 2022/10/21 | [Code](https://github.com/yueyang1996/z-lavi)          |
| Imagination-Augmented Natural Language Understanding         | https://aclanthology.org/2022.naacl-main.326/     | 2022/07    | [Code](https://github.com/yujielu10/iace-nlu)          |
| Visually-Augmented Language Modeling                         | https://arxiv.org/abs/2205.10178                  | 2022/05/20 | [Code](https://github.com/Victorwz/VaLM)               |
| Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer | https://arxiv.org/abs/2203.07519                  | 2022/05/14 |                                                        |
| Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation | https://arxiv.org/abs/2109.05778                  | 2021/09/13 |                                                        |
| Maria: A Visual Experience Powered Conversational Agent      | https://arxiv.org/abs/2105.13073                  | 2021/05/27 | [Code](https://github.com/jokieleung/Maria)            |
| Experience Grounds Language       |       https://aclanthology.org/2020.emnlp-main.703/         | 2020/11 |           |
| Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision | https://arxiv.org/abs/2010.06775                  | 2020/10/14 | [Code](https://github.com/airsplay/vokenization)       |
| Open Domain Dialogue Generation with Latent Images           | https://arxiv.org/abs/2004.01981                  | 2020/04/04 |                                                        |

## Other Lists

- [BradyFU/Awesome-Multimodal-Large-Language-Models: :sparkles::sparkles:Latest Papers and Datasets on Multimodal Large Language Models, and Their Evaluation.](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

- [Atomic-man007/Awesome_Multimodel_LLM: Awesome_Multimodel is a curated GitHub repository that provides a comprehensive collection of resources for Multimodal Large Language Models (MLLM). It covers datasets, tuning techniques, in-context learning, visual reasoning, foundational models, and more. Stay updated with the latest advancement.](https://github.com/Atomic-man007/Awesome_Multimodel_LLM)
- [ImKeTT/Awesome-Multi-Modal-Dialog: Paperlist Awesome paper list of multimodal dialog, including methods, datasets and metrics](https://github.com/ImKeTT/Awesome-Multi-Modal-Dialog)
